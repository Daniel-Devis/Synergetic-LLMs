üîç Self-Reviewing LLMs for Unit Test Generation
This project investigates the capabilities of large language models (LLMs) ‚Äî specifically GPT-3.5 Turbo and GPT-4o Mini ‚Äî to automatically generate and self-review unit tests for Java methods using a dual-LLM framework. We evaluate how effective these models are when given both the task of test generation and subsequent review, without human intervention.

üß† Project Overview
Fine-tune LLMs on paired Java focal methods and reference unit tests from the Methods2Test dataset.

Generate test cases for unseen methods using the fine-tuned models.

Use the same or a different LLM to critically evaluate the generated test cases.

Assess the performance using metrics such as CodeBLEU, Precision, Recall, and F1 Score.

üóÇÔ∏è File Descriptions
Filename	Description
3.5finetune.py	Calls the OpenAI API to fine-tune GPT-3.5 Turbo on JSONL files containing prompt-completion pairs.
4ominifinetune.py	Same as above, but fine-tunes GPT-4o Mini.
3.5generator.py	Uses the fine-tuned GPT-3.5 Turbo model to generate unit tests for Java focal methods.
4ogptmini.py	Uses the fine-tuned GPT-4o Mini model for the same purpose.
3.5testgencheck.py	Evaluates test cases generated by GPT-3.5 using CodeBLEU, Precision, Recall, and F1. Also includes a second LLM review step.
4ogpttestgencheck.py	Same as above, for GPT-4o Mini outputs.
jasonifier.py	Constructs the fine-tuning file using focal methods and their correct reference test cases.
fulevalcsv.py	Aggregates and outputs evaluation results into a CSV file with columns: Filename, Average CodeBLEU Score, Average Precision, Average Recall, Average F1 Score.
testv2.py	Experimental script to test different parts of the pipeline (e.g., generation or review in isolation).

üìä Dataset
This project uses the Methods2Test dataset, which provides:

Java methods (focal methods)

Reference unit tests

Class and method structure annotations
